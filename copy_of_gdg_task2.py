# -*- coding: utf-8 -*-
"""Copy of GDG_Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JoWEqZ9mtp2SftxCyf7oeXt40i3T4IS4

#**GDG TASK 2**

![GDG On Campus - Centered - 10x10 White.png](data:image/png;base64,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)

Last week, we dealt with the visualization and analytical aspect of ML. This week, lets directly get our hands dirty with various ML models.

Possibly the first question one must ask is, what is Machine Learning essentially made up of? In a broad sense, ML problems can be split into Supervised and Unsupervised learning. In supervised learning are the categories of Classification and Regression, which is what we'll explore today.

Like last time, resources and comments are provided above each cell, so peruse the resources and then get coding.

Supervised vs Unsupervised vs Reinforcement Learning:

https://www.simplilearn.com/tutorials/machine-learning-tutorial/types-of-machine-learning

Regression vs Classification :

https://www.analyticsvidhya.com/blog/2023/05/regression-vs-classification/ (Might be a little wordy)

https://www.youtube.com/watch?v=1NBwM5tavTk&ab_channel=IntuitiveML
(A very quick video)

Machine Learning for Everyone (Read till 1.1):

https://vas3k.com/blog/machine_learning/

OPTIONAL BUT USEFUL <br>
Overfitting and Underfitting [VERY IMPORTANT]
https://www.youtube.com/watch?v=T9NtOa-IITo

Lets import all the basic libraries.
"""

!pip install shap -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""##Regression

For regression, lets reuse the crab dataset that you worked on last week. Import the dataset from wherever you had saved it in the cell below.
Also print the first 5 rows to ensure you have loaded the dataset correctly.
"""

df_crab = pd.read_csv('/content/Task1.csv')
df_crab.head()



"""Now, let's some employ some Feature Engineering tactics which you would have performed in the previous tasks.

Create a function called crab_processing which takes a dataframe as an input, performs the following tasks, and returns a cleaned and edited df:

1) Drop the 'id' column.

2) Drop all rows where 'Height' of the crab is 0.

3) Create a new column called 'LostWeight' which is calculated using the following formula - ```
Weight - (Shucked + Viscera + Shell)``` .
If this formula gives a number less than 0, then assign 0 to the LostWeight column, else assign 1.

4) Finally, use one hot encoding to encode the column 'Sex'

Take help from your previous task where you have implemented these steps in different cells.

"""

def crab_processing(df):
    # 1) Drop the 'id' column.
    df = df.drop('id', axis=1)

    # 2) Drop all rows where 'Height' of the crab is 0.
    df = df[df['Height'] != 0]

    # 3) Create a new column called 'LostWeight'
    df['LostWeight'] = df['Weight'] - (df['Shucked Weight'] + df['Viscera Weight'] + df['Shell Weight'])
    df['LostWeight'] = df['LostWeight'].apply(lambda x: 0 if x < 0 else 1)

    # 4) Use one hot encoding to encode the column 'Sex'
    df = pd.get_dummies(df, columns=['Sex'], drop_first=True)

    return df

df_crab = crab_processing(df_crab)
df_crab.head()

"""If all is correct, this should be the shape of your df now."""

df_crab.shape

"""###Basic Visualization

In this problem, our aim is to predict the age of the crab, hence 'Age' here is called our target variable. To check its distribution, plot a histogram of the 'Age' column.

Choose the number of bins by experimenting. The output is expected to mimic a bell curve.

What is a histogram? - https://www.w3schools.com/python/matplotlib_histograms.asp

Difference between histogram and a bar graph - https://keydifferences.com/difference-between-histogram-and-bar-graph.html
"""

plt.figure(figsize=(10, 6))
sns.histplot(df_crab['Age'], bins=30, kde=True)
plt.title('Distribution of Crab Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

"""###Feature and Variable Sets

As mentioned above, the 'Age' column is what we're gonna be predicting. So essentially we need to create two additional dataframes, one which contains all the training features (All columns except 'Age') and one which only contains the column 'Age'. Name them X and y respectively.
"""

X = df_crab.drop('Age', axis=1)
y = df_crab['Age']

y.isnull().sum()

print(X.shape, y.shape)

display(X.head())

display(y.head())

"""###Train-Test Split

Implementation - https://www.youtube.com/watch?v=BUkqYGPnLZ8&ab_channel=ManifoldAILearning

Now, while we need data to train our regression model, it is equally important to keep some data aside for testing the effectiveness of the aforementioned model. Thus the dataset as a whole is generally further divided into the training dataset and the testing dataset.

In order to implement this, import train_test_split function from scikit-learn.
"""

from sklearn.model_selection import train_test_split

"""**Create X and y train and test splits in one command using a test size of 0.3 and a random seed**

They should be called X_train, X_test, y_train, y_test
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Print the size and shape of each of the train/test splits (it should be in the ratio as per test_size parameter above, i.e in ratio of 0.3)"""

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""Can you see that the data has been divided into 2 datasets of size 70% and 30% of the original dataset each? Use your calculator to confirm this :)

###Model fit and training

Here is the complete lowdown on SVM (Support Vector Machine):<br>
Theory : https://youtu.be/H9yACitf-KM <br>
Theory: https://www.analyticsvidhya.com/blog/2021/06/support-vector-machine-better-understanding/ <br>
Implementation : https://youtu.be/FB5EdxAGxQg

Import SVR (Support Vector Regressor) and its metrics from scikit-learn.
"""

from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

svr = SVR()

"""Fit the model on to the instantiated object itself using the X_train and y_train created earlier. No need to create another variable.

Hint: svr.fit()

This might take about a minute or so, just to inform you.
"""

svr.fit(X_train, y_train)



"""###Prediction, error estimate, and regression evaluation matrices

**Prediction using the svr model**

The X_train and y_train dataframes have been used to train the model. Now we will use X_test and y_test to evaluate the efficiency of the model we have trained.

Use svr.predict() on X_test and store it in a variable called "predictions". Print type and size of the predictions.

Size should be (22201,) if everything is correct.
"""

predictions = svr.predict(X_test)
print(f"Type of predictions: {type(predictions)}")
print(f"Size of predictions: {predictions.shape}")



"""Now that we have our predictions, let's compare it with y_test and see how accurate our predictions are.

Plot a Scatter plot of predicted price and y_test set to see if the data falls on a 45 degree straight line
"""

plt.figure(figsize=(10, 6))
plt.scatter(y_test, predictions, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Age')
plt.ylabel('Predicted Age')
plt.title('Actual vs Predicted Age (SVR)')
plt.grid(True)
plt.show()

"""As you can see, it's not perfect, but you can definitely visualize the data lining up at a 45 degree angle.

**Model Evaluation**

We will be exploring the most common regression metric here namely R2 score. You can check out more metrics here: https://scikit-learn.org/stable/modules/classes.html#regression-metrics

Here's a video detailing all the popular regression metrics - https://www.youtube.com/watch?v=Ti7c-Hz7GSM

**Print the R-square value and round it to 3 decimal places**

Hint: sklearn metrics.r2_score
"""

r2 = r2_score(y_test, predictions)
print(f"R-squared value: {r2:.3f}")

"""In the cell below, explain what the above score means and also elaborate how 'good' is the score in terms of evaluation of the model

An R² score of 0.54 means the model explains 54% of the variance in the target variable, leaving 46% unexplained. This indicates moderate performance—better than random guessing but far from ideal.
"""

mae = mean_absolute_error(y_test, predictions)
print(f"Mean Absolute Error: {mae:.3f}")

mse = mean_squared_error(y_test, predictions)
print(f"Mean Squared Error: {mse:.3f}")

rmse = np.sqrt(mse)
print(f"Root Mean Squared Error: {rmse:.3f}")

"""You have succesfully implemented Support Vector Regressor to predict Age of a crab. But is this the only regression model out there? In the cell below, list down 3 more regression models with a basic explanation of how they work. Also include a line explaining in which scenario each model works best.

**1. XGBoost Regressor**

**How it works**: XGBoost is an ensemble method based on decision trees. It builds trees sequentially, where each tree corrects the errors of the previous ones, and uses gradient descent to minimize errors efficiently.

    Best scenario:


**2. Random Forest Regressor**

**How it works**: An ensemble method that constructs multiple decision trees during training. It combines their outputs averages them to improve prediction accuracy and control overfitting.

    Best scenario:


**3. Ridge Regressor**

**How it works**: A type of linear regression that adds an L2 regularization term to the cost function, penalizing large coefficients to prevent overfitting.

    Best scenario:

###Explainability

So you passed some data into a model, and trained it using that data. You then used some other data to test the accuracy of that model, and it now predicts values when you pass in some data. Ok. But how does it REALLY predict that value? How much is it relying on each individual feature column of data that you passed to the model? This is where the concept of explainability comes into picture, where you can understand for each data point, exactly what parameters led the model to predict the value that it has.

Info about explainable AI <br>
https://www.ibm.com/topics/explainable-ai <br>
About shap <br>
https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html
"""

#Run this code as it is
from shap import KernelExplainer, force_plot, sample

#Run this code as it is
ex = KernelExplainer(svr.predict, sample(X_train, 10))  #only taking sample of 10 values for now
shap_values = ex.shap_values(X_test.iloc[0,:]) #explaining 1st tuple of the X_test dataframe
force_plot(ex.expected_value, shap_values, X_test.iloc[0,:], matplotlib=True)

"""Can you see how the above plot visualizes which parameters contributed significantly to the prediction, and what were the actual values which led to the prediction?

##Classification

In the below cells, we are going to implement K Nearest Neighbour Classification.

Link to the dataset is here - https://drive.google.com/file/d/1LlLZM-DCdajm9-EakRLM7qJSYQghJZUW/view?usp=sharing <br>

Import the 'classified_data.txt' dataset using pandas and print the first five columns.

Use "index_col" parameter to index the dataframe according to the first column. Otherwise, a new column would get created. Call this new dataframe 'df'.
"""

df = pd.read_csv('/content/Task2.csv',index_col=0)
df.head()

"""If you're wondering what these column names signify, well they are classified as the dataset name suggests. Our job here is to simply take the data at face value and build a model for classifying the target classes.

###Basic EDA

In the following two cells, use info() and describe() to get the data type and statistical summary of the dataset
"""

df.info()

df.describe()

"""**Run a 'for' loop to draw boxplots of all the features for '0' and '1' TARGET CLASS**<br>
Hint: Loop through each of the 10 features and draw a separate boxplot. You should have 10 boxplots in total. <br>
Refer seaborn boxplot() documentation

Here is some information about boxplots:

Understanding Boxplots:
https://medium.com/analytics-vidhya/introduction-to-box-plots-and-how-to-interpret-them-22464acbcba7 <br>
Comparing two boxplots: https://www.nagwa.com/en/explainers/812192146073/
"""

features = df.columns.drop('Age') # All columns except 'Age'

for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Age', y=feature, data=df)
    plt.title(f'Boxplot of {feature} by Age')
    plt.show()

"""What conclusion, if any can you draw from these boxplots?
List them down below.

Text here

### Standardize the features using sklearn.preprocessing
Why should we standardize?<br>
Variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias. Thus, to deal with this potential problem feature-wise standardization is usually used prior to model fitting.<br><br>

To give you an example, values pertaining to Age usually lie within the range of 1-100, however values pertaining to Salary lie in a much wider range of say 10000-1000000. You can see how having both columns be a part of a classification model as is would lead to an imbalance in importance assigned to columns.
<br><br>
Go through this link for a better understanding:<br>
https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832

**import StandardScaler from Sklearn and instantiate it to a variable called "scaler"**
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

"""**Fit only the features data to this scaler (leaving the TARGET CLASS column out) and then transform**<br>
Hint: scaler.fit() and scaler.transform()
"""

# Apply one-hot encoding to the 'Sex' column
df_processed_classification = pd.get_dummies(df, columns=['Sex'], drop_first=True)

scaled_features = scaler.fit_transform(df_processed_classification.drop('Age', axis=1))

"""**scaler.transform() will return an array. We need to convert this into a dataframe. Do this and add the column names to the dataframe. Call this new dataframe as "df_feat". Call head() on this df**<br>
Note: The final dataframe will have the initial columns except the "TARGET CLASS".
"""

df_feat = pd.DataFrame(scaled_features, columns=df_processed_classification.columns.drop('Age'))
display(df_feat.head())

"""### Train/Test split

**Set X to be equal to df_feat and set y accordingly. As you know, X contains our training features and y contains our target.**<br>
Hint: y can be taken directly from the initial dataframe "df"
"""

X = df_feat
y = df_processed_classification['Age']

"""**Import train_test_split function from scikit-learn**<br>
**Create X and y train and test splits in one command using a test size of 0.3 and a random seed**<br>
They should be called X_train, X_test, y_train, y_test
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### Model fit and training

Before moving ahead with the cells below, learn how the K Nearest Neighbour Algorithm actually works.

Theory and implementation : https://youtu.be/wTF6vzS9fy4 <br>
Theory: https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/

**import KNeighborsClassifier from sklearn and initialize it with neighbours = 1 (more on this later) . Fit this on X_train and y_train**
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

"""**Using this fitted model, predict on X_test. Store these predictions in variable called pred.**"""

pred = knn.predict(X_test)

"""**Let us check how correct these predictions are.<br>
Print a classification report of y_test and pred**<br>
Hint: sklearn classification_report
"""

from sklearn.metrics import classification_report

print(classification_report(y_test, pred))

"""Learn all about classification metrics - https://medium.com/analytics-vidhya/evaluation-metrics-for-classification-models-e2f0d8009d69

**Print the accuracy using numpy and round it to 3 decimal places.**
"""

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, pred)
print(f"Accuracy: {accuracy:.3f}")

"""Hey, not bad! But can we further improve this? Remember our arbitrary choice for n_neighbours? Can we improve that?

### Choosing optimal 'k'

**Above, we chose n_neighbours to be equal to 1. Choosing a small value of K leads to unstable decision boundaries. <br>
We need to select n_neighbours by calculating the accuracy for every value of n from 1 to 60 and then choosing the one which gives the highest accuracy.**
"""

accuracy_scores = []

for i in range(1, 61):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    accuracy_scores.append(accuracy_score(y_test, pred_i))

"""**Plot a graph of K value vs Accuracy**"""

plt.figure(figsize=(12, 6))
plt.plot(range(1, 61), accuracy_scores, marker='o', linestyle='--')
plt.title('Accuracy vs. K Value')
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Accuracy')
plt.xticks(np.arange(1, 61, 2))
plt.grid(True)
plt.show()

"""**Choose the best value of n_neighbours and give a reason why and also print the accuracy**"""

optimal_k = np.argmax(accuracy_scores) + 1
best_accuracy = accuracy_scores[optimal_k - 1]

print(f"The best value for n_neighbors (k) is: {optimal_k}")
print(f"The highest accuracy achieved is: {best_accuracy:.3f}")

# Re-train with optimal K
knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k)
knn_optimal.fit(X_train, y_train)
pred_optimal = knn_optimal.predict(X_test)
print(f"Classification Report with optimal K={optimal_k}:")
print(classification_report(y_test, pred_optimal))

# Reasons:
# The graph of K value vs Accuracy shows that the accuracy generally increases up to a certain point
# and then starts to fluctuate or decrease. The 'optimal_k' determined programmatically is the point
# where the highest accuracy was recorded. Choosing this 'k' value aims to strike a balance between
# bias and variance, leading to the best generalization performance for this dataset using KNN.

"""# **Compulsory:**

Task 1 : You have to research and create a short blog on the topic of **Reinforcement Learning**.


Task 2 : You have to research and create a short blog on the topic of **Oversampling and Undersampling**.
Happy coding! :)

# **Compulsory:**

Task 1 : You have to research and create a short blog on the topic of **Reinforcement Learning**.

Task 2 : You have to research and create a short blog on the topic of **Oversampling and Undersampling**. Happy coding! :)

### **Blog Post 1: Reinforcement Learning - Learning by Doing**

Reinforcement Learning (RL) is a fascinating area of machine learning where an agent learns to make decisions by interacting with an environment. Unlike supervised learning, which relies on labeled data, or unsupervised learning, which finds patterns in unlabeled data, RL is all about learning through trial and error, similar to how humans or animals learn.

**How it Works:**
At its core, RL involves:

*   **Agent:** The learner or decision-maker.
*   **Environment:** The world the agent interacts with.
*   **State:** The current situation of the agent within the environment.
*   **Action:** What the agent chooses to do in a given state.
*   **Reward:** A feedback signal from the environment, indicating how good or bad an action was.
*   **Policy:** The strategy the agent uses to decide its next action based on the current state.

The agent takes an action in a given state, receives a reward (or penalty), and transitions to a new state. The goal of the agent is to learn a policy that maximizes the cumulative reward over time. This is often achieved through techniques like Q-learning or Deep Q-Networks (DQNs), where the agent learns the 'value' of taking certain actions in certain states.

**Why is it powerful?**
RL excels in scenarios where explicit programming is difficult or impossible. It allows systems to discover optimal strategies in complex, dynamic environments. Think of games like Chess or Go, controlling robotic arms, or optimizing resource allocation.

**Real-world Applications:**
*   **Robotics:** Learning complex motor skills.
*   **Game Playing:** Achieving superhuman performance in video games and board games.
*   **Autonomous Driving:** Making real-time decisions in traffic.
*   **Resource Management:** Optimizing energy consumption or network traffic.

RL is a rapidly evolving field with immense potential to create truly intelligent and adaptive systems that can learn and improve on their own.

### **Blog Post 2: Oversampling and Undersampling - Balancing Your Data for Better Models**

In machine learning, one common challenge is dealing with **imbalanced datasets**. This occurs when the number of observations for one class is significantly lower than for other classes. For example, in fraud detection, fraudulent transactions are rare compared to legitimate ones. If not handled, this imbalance can lead machine learning models to be biased towards the majority class, performing poorly on the minority class which is often the one we care about most.

To tackle this, two popular techniques are **Oversampling** and **Undersampling**.

**1. Oversampling:**

Oversampling involves increasing the number of instances in the minority class to balance the dataset. Instead of losing information, you're creating more.

*   **How it works:** You duplicate existing examples from the minority class, or, more commonly, generate synthetic examples. A popular method for generating synthetic examples is **SMOTE (Synthetic Minority Over-sampling Technique)**, which creates new samples along the line segments joining minority class samples and their nearest neighbors.

*   **When to use it:** Ideal when you have limited data for the minority class and want to ensure the model has enough examples to learn its characteristics. It helps prevent information loss.

*   **Potential drawback:** Can lead to overfitting if not used carefully, as duplicating or synthetically generating too many similar examples might make the model memorize the training data rather than generalize.

**2. Undersampling:**

Undersampling, on the other hand, reduces the number of instances in the majority class to achieve balance.

*   **How it works:** You randomly remove examples from the majority class until the class distribution is more balanced. There are also more sophisticated methods like Tomek Links or Edited Nearest Neighbors, which remove majority class samples that are

too close

to minority class samples, making the decision boundary clearer.

*   **When to use it:** Useful when you have a very large dataset, and reducing the number of samples in the majority class doesn't lead to significant information loss. It can also help speed up training times.

*   **Potential drawback:** Can lead to information loss from the majority class, potentially causing the model to miss important patterns in the larger class.

**Which one to choose?**
Often, a combination of both oversampling and undersampling techniques yields the best results. The choice depends on the dataset size, the degree of imbalance, and the specific problem you're trying to solve. Experimentation is key to finding the right balance for your machine learning model.

# **End of Task**
"""