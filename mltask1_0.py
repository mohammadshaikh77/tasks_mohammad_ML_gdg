# -*- coding: utf-8 -*-
"""mltask1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rZJ7xbC4VHeT8bfmfqJeoUqRd6MHRgQ7

#**GDG TASK 1**


Welcome to GDG!

Your first task in this committee is to clear your Exploratory Data Analysis (EDA) concepts. We'll start from the very basics, and make it just a teeny lil bit more complex with every next code cell.

I suppose a nice place to start with would be the definition of EDA. Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations. It is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualisation methods.

Above each code cell, instructions and resources have been given. Go through the resources, then implement the code accordingly. Feel free to add extra cells to play around on your own as well :p

#installing libraries and importing them

Some libraries like numpy and pandas are already pre installed on Colab. Some need to explicitly installed. A really cool data exploration library called ydata-profiling falls in the latter category. So we use the appropriate pip command to install it.
"""

!pip install ydata-profiling -q

"""Mount your Google Drive onto this notebook."""

# write code here
from google.colab import drive
drive.mount('/content/drive')

"""Import basic libraries."""

import ydata_profiling as pp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""#loading the dataset

The dataset we wanna nitpick apart this week is one pertaining to crabs. Use this link to download it:

https://drive.google.com/file/d/14384FUrzE1gB7HWn8GmAJe3hfx6Glv8R/view?usp=sharing

Upload it to your Google Drive. Then read it into a variable using the pandas library.
"""

# read dataset into a variable
dataset=pd.read_csv("/content/Task1.csv")

"""View the first 5 rows of this DataFrame to see if it has been stored correctly."""

# write code here
dataset.head()

"""Right, so now you have your entire DataFrame residing in a variable. But again, what even is a DataFrame? Just speed through the below article.

https://www.databricks.com/glossary/what-are-dataframes

#basic EDA

Here, we aim to get a basic overview of the dataset.

List the columns in this dataset.
"""

# write code here
print(dataset.columns)

"""You may have noticed that the 'id' column is just indexing the rows in the DataFrame. But we don't really need that, as pandas very neatly handles that for us. So drop that column."""

# drop the column, then view its first 5 values
dataset=dataset.drop('id',axis=1)

"""Display the the number of rows and columns in this dataset."""

# write code here
dataset.shape

"""Use the info() function to get, well, info about it."""

# write code here
dataset.isnull().sum()
dataset.info()

"""If done correctly, you'll encounter no null values. How lucky.

What are the statistics of this data? Use describe() function to view them. Google what each of those row headers mean.
"""

# write code here
dataset.describe()

"""count gives the total number of non null entry

std is the standard devivation

* mean doesnot tell about the distribution

* example 3,3,3 and 2,3,4 mean is 3 of both but the spread is different

* then the variance is used to calcuate spread but as it is square not in the dimension of the data therefore we use std which is square root of the varianace.

mean is average

**last 5 rows states about the five-number summary**

Min - Lowest value

Q1 (25%) - Lower quartile(basically value at 25 percentile)

Median (50%) - Middle value

Q3 (75%) - Upper quartile(basically value at 75 percentile)

Max - Highest value

Write a function to print all the unique values in the columns of Sex and Age.
"""

# write code here
print(dataset['Sex'].unique())

print(dataset['Age'].unique())

"""List out the number of crabs belonging to each gender."""

# write code here
print(dataset['Sex'].value_counts())

"""If your output is correct, you'll find the dataset is decently well-balanced, although it is a bit skewed towards male crabs.

#pandas profiling

Doing all the above for each dataset in separate cells gets boring fast. Enter ydata_profiling. You just have to feed a DataFrame to it, and it takes care of basic EDA (and then some more) for you. Resource:

https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html

Run a Profile Report on our dataframe and just go through it.
"""

# write code here
profile=pp.ProfileReport(dataset,title="report")
profile.to_notebook_iframe()

"""A lot of what we did above manually gets handled automatically. Pretty cool, innit?

Also, if you genuinely went through the report, you'll find that some crabs have height as zero. That doesn't make much sense.

Drop all these apparently two dimensional crabs from the dataset. Get rid of all rows where height is zero.
"""

# drop the necessary rows as asked above
dataset = dataset[dataset['Height'] != 0]

"""If done correctly, your DataFrame now will have shape (74027, 9). Check that below."""

# write code here
dataset.shape

# as we drop some columns to make index again normal
dataset = dataset.reset_index(drop=True)

"""#plotting some stuff out

To explore data properly, data visualisation techniques are employed. What that essentially means that we're gonna plot some graphs and glean meaningful insights from them. We use the matplotlib library for this, and seaborn to make it look real pretty, for no one likes an ugly graph.

#bar charts

Plot a bar graph to view the average age of each sex. For help, refer:

https://www.analyticsvidhya.com/blog/2021/08/understanding-bar-plots-in-python-beginners-guide-to-data-visualization/
"""

# write the logic here before you actually plot the graph
avg_age_by_sex=dataset.groupby('Sex')['Age'].mean()

"""x axis will contain Sex, and y axis has the average age."""

# plot the graph here
avg_age_by_sex.plot(kind='bar')
plt.xlabel('Sex')
plt.ylabel('Average Age')
plt.title('Average Age by Sex')
plt.show()

"""As with humans, it seems that crabs have their female sex having longer lives on average. Still, all genders have a pretty short lifespan. Shame.

Anyways, the above bar graph of Average Age and Sex is not the most comprehensive way to analyse such data.

#box plots

To get what I mean, make a boxplot of Age and Sex. Again, refer to these before you code:

https://www.youtube.com/watch?v=Vo-bfTqEFQk

https://builtin.com/data-science/boxplot
"""

# plot the graph here
import matplotlib.pyplot as plt

# Boxplot of Age distribution by Sex
plt.figure(figsize=(8,6))
dataset.boxplot(column='Age', by='Sex')
plt.xlabel('Sex')
plt.ylabel('Age')
plt.title('Box Plot of Age by Sex')
plt.suptitle('')
plt.show()

"""Bar charts usually just tell you the count of some items in a dataset. While that is useful, it doesn't really say much about the distribution of those items in that dataset. That's where boxplots come in: to tell you the characteristics of data. For more information:

https://www.nature.com/articles/nmeth.2807

#KDE plots

Now, we'll graph some of the other numeric data with age to try and get more information about it all relates. Display KDE plots of Age, Length, and Weight. For resources about a KDE plot:

https://www.youtube.com/watch?v=DCgPRaIDYXA

https://datagy.io/seaborn-kdeplot/

Don't make three separate cells for each of these plots. All three plots should be visible in a single row.
"""

# write code that yields output similar to the one shown below
import seaborn as sns
import matplotlib.pyplot as plt



plt.figure(figsize=(10,6))

# KDEs for each feature
sns.kdeplot(dataset['Age'], label='Age')
sns.kdeplot(dataset['Length'], label='Length')
sns.kdeplot(dataset['Weight'], label='Weight')

plt.xlabel('Value')
plt.ylabel('Density')
plt.title('KDE Plot: Age, Length, and Weight')
plt.legend()
plt.show()

"""#scatter plots

Make a scatterplot between Age and Diameter. Resources for the same:

https://www.youtube.com/watch?v=4yz4cMXCkuw

https://www.cuemath.com/data/scatter-plot/
"""

# plot the graph here
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(dataset['Diameter'], dataset['Age'], alpha=0.5)
plt.xlabel('Diameter')
plt.ylabel('Age')
plt.title('Scatter Plot: Age vs Diameter')
plt.show()

"""# some more complex analysis

Use a function called corr(). Resource:

https://data36.com/correlation-definition-calculation-corr-pandas/
"""



# write code here
correlation_matrix = dataset.drop('Sex', axis=1).corr()
print(correlation_matrix)

"""If you've been paying attention, the Profile Report above already took care of this. This table basically tells you how every column correlate with each other. Closer the number is to 1, the more they mirror each other.

From this report, it can be seen that Weight is extremely highly correlated with each of shucked weight, viscera weight and shell weight.

To actually understand what this means, we first need to know what all this crab jargon actually is. So, on Googling, you will discover the following meanings:

*   Weight - total teight
*   Shucked - weight of meat only
*   Viscera - gut weight, after bleeding
*   Shell - weight after being dried

Well, those are some pretty disgusting and gory definitions.

Moving past that, we here find that some weight statistics is lost. What about the other potential body parts of a crab? Maybe we need them to accurately predict its (apparently very short) lifespan? Maybe it is actually needed in some future model we choose to implement?

So, make a new column called "Lost Weight". Use the following formula to populate every row of it:



```
Weight - (Shucked + Viscera + Shell)
```

If this operation gives a value of Lost Weight that is less than or equal to zero, then assign zero to that row's Lost Weight value. Otherwise, assign one to it.

Also, ensure that this column is added in between Shell Weight and Age.
"""

import numpy as np
lost_weight = dataset['Weight'] - (
    dataset['Shucked Weight'] +
    dataset['Viscera Weight'] +
    dataset['Shell Weight']
)
dataset['Lost Weight'] = np.where(lost_weight <= 0, 0, 1)
shell_idx = dataset.columns.get_loc('Shell Weight')
if 'Lost Weight' in dataset.columns:
    dataset = dataset.drop(columns=['Lost Weight'])
dataset.insert(shell_idx + 1, 'Lost Weight', np.where(lost_weight <= 0, 0, 1))

dataset.head()



"""To see how the crabs having no Lost Weight compare with those having some with respect to age, lets draw a violin plot. Resources:

https://www.youtube.com/watch?v=PNNLefP974M

https://seaborn.pydata.org/generated/seaborn.violinplot.html

Code it below now. Here, x is the Lost Weight, and y is the Age.
"""

# plot the graph here
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
sns.violinplot(x='Lost Weight', y='Age', data=dataset)
plt.xlabel('Lost Weight (0 = None, 1 = Some)')
plt.ylabel('Age')
plt.title('Violin Plot of Age by Lost Weight')
plt.show()

"""#make a plot of your own choosing!

Below is a link to the gallery of various plots available in seaborn.

https://seaborn.pydata.org/examples/index.html

You have free rein to make any type of graph (that should not be what we have exactly done above), between any parameters you want to compare. Make it real eye candy to look at, and also below that plot write what relevant insight you obtained from it.
"""

# creativity batao
import seaborn as sns
import matplotlib.pyplot as plt


pairplot_cols = ['Age', 'Length', 'Diameter', 'Weight', 'Shell Weight', 'Lost Weight']

sns.pairplot(dataset[pairplot_cols], hue='Lost Weight', palette='coolwarm', diag_kind='kde', plot_kws={'alpha': 0.5})
plt.suptitle('Pairplot: Numeric Features colored by Lost Weight', y=1.02)
plt.show()

"""Each diagonal shows smoothed (KDE) histograms for that feature by Lost Weight group.

Each off-diagonal is a scatter plot, highlighting dependencies between feature pairs, colored by Lost Weight (so trends become instantly visible for "no loss" vs "some loss").

You can spot hidden clusters, relationships, or patterns your earlier graphs might have missedâ€”simultaneously across many features.

Overlapping and divergence between Lost Weight classes become visually obvious.

This heatmap shows the correlation of different features with "Age". Key insights:

1. **Shell Weight** has the highest correlation with **Age** (0.66), making it a strong predictor.
2. **Height**, **Diameter**, **Length**, and **Weight** have moderate correlations, making them useful for prediction.
3. **Lost_Weight** has a low correlation (0.15), suggesting it may be less useful as a feature.

 It helps quickly identify important features for machine learning by showing correlations with the target variable.
"""

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numeric Features')
plt.show()

"""#one hot encoding

Machine Learning algorithms can only crunch numbers. Give it a string or text input, and it self-annihilates by throwing an error. To handle this, we use a technique called one hot encoding. Read up on it here:

https://datagy.io/pandas-get-dummies/

Then, apply it to the column titled "Sex."
"""

dataset = pd.get_dummies(dataset, columns=['Sex'])

display(dataset.head())



"""If correctly implemented, you will observe three new columns are added, namely, "Sex_F", "Sex_I" and "Sex_M." The old "Sex" column is now gone.

#normalisation and standardisation

To prevent some data features from dominating the model training process, we implement the above two feature scaling techniques.

Go through the below resource:

https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/

Normalise the dataset.
"""

df_normalized = dataset.copy()
for column in df_normalized.columns:
    if df_normalized[column].dtype != 'bool':
        df_normalized[column] = (df_normalized[column] - df_normalized[column].min()) / (df_normalized[column].max() - df_normalized[column].min())

display(df_normalized.head())

"""For standardisation, bear in mind that the one hot encoded columns are not standardised. They are categorical in nature, so it makes no sense to shoehorn them into any type of distribution at all.

But, its ok to normalise them, as it will be scaled down to values between 0 and 1, which is the range in which they already exist anyways. Normalisation doesn't affect them.
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Identify numeric columns for standardization (excluding one-hot encoded columns)
numeric_cols = dataset.select_dtypes(include=np.number).columns.tolist()
one_hot_cols = [col for col in numeric_cols if col.startswith('Sex_')]
cols_to_standardize = [col for col in numeric_cols if col not in one_hot_cols]

df_std = dataset.copy()
df_std[cols_to_standardize] = scaler.fit_transform(df_std[cols_to_standardize])

display(df_std.head())

"""Standardise df_std such that the one hot label encoded columns aren't affected."""

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False)

display(df_normalized_sklearn.head())

# Execute the code from cell kE_k25XR1qcK to define df_normalized_sklearn
df_normalized_sklearn = dataset.copy()

numeric_cols = df_normalized_sklearn.select_dtypes(include=np.number).columns.tolist()
one_hot_cols = [col for col in numeric_cols if col.startswith('Sex_')]
cols_to_normalize = [col for col in numeric_cols if col not in one_hot_cols]

df_normalized_sklearn[cols_to_normalize] = scaler.fit_transform(df_normalized_sklearn[cols_to_normalize])

for col in one_hot_cols:
    df_normalized_sklearn[col] = df_normalized_sklearn[col].astype(int)

# Now display the head of the DataFrame
display(df_normalized_sklearn.head())

df_normalized_sklearn = dataset.copy()

numeric_cols = df_normalized_sklearn.select_dtypes(include=np.number).columns.tolist()
one_hot_cols = [col for col in numeric_cols if col.startswith('Sex_')]
cols_to_normalize = [col for col in numeric_cols if col not in one_hot_cols]

df_normalized_sklearn[cols_to_normalize] = scaler.fit_transform(df_normalized_sklearn[cols_to_normalize])

for col in one_hot_cols:
    df_normalized_sklearn[col] = df_normalized_sklearn[col].astype(int)

"""#**Compulsory: You can research on the topic of One Hot Label Encoding vs Label Encoding only if you are done with the task. :)**"""